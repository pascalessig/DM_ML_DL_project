# -*- coding: utf-8 -*-
"""evaluation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UdfdU XA66YURQgtPhs-J8263Ygy1lEA4
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.metrics import matthews_corrcoef
import warnings
import seaborn as sns
warnings.filterwarnings('ignore')

sns.set()

def print_acc_measures(y_test, y_score):
  """
  Calculating the evaluation measures accuracy, f1_score, precision, recall
  And bring them in a nicely readyble format

  Arguments:
  y_test      -- the true classses of the test set obesrvations (np integer array shape [len(y_test), 1])
  y_score_c   -- the class labels the classifier assigns to the observations in the test set (np integer array shape [len(y_test),])
  """
  correct = (y_test == y_score).sum()
  tp = ((y_test == 1) & (y_score == 1)).sum()
  tn = ((y_test == 0) & (y_score == 0)).sum()
  fp = ((y_test == 0) & (y_score == 1)).sum()
  fn = ((y_test == 1) & (y_score == 0)).sum()
  precision = tp / (tp + fp)
  recall = tp / (tp + fn)
  acc = correct / y_test.shape[1]
  mcc = matthews_corrcoef(y_test.reshape(-1), y_score.reshape(-1))
  print('---------------Evaluation-Classifier--------------')
  try:
      f1 = 2 * precision * recall / (precision + recall)
  except ZeroDivisionError:
      print('\nThe classifier achieves a overall accuracy of %0.4f on the test set.\n\nFurther details loading...\n\nTP: %i\nTN: %i\nFP: %i\nFN: %i\n\nF1-Score: %0.2f\nPrecision: %0.2f\nRecall: %0.2f\nMCC: %0.2f'%(acc, tp, tn, fp, fn, f1, precision, recall, mcc))


  print('\nThe classifier achieves a overall accuracy of %0.4f on the test set.\n\nFurther details loading...\n\nTP: %i\nTN: %i\nFP: %i\nFN: %i\n\nF1-Score: %0.2f\nPrecision: %0.2f\nRecall: %0.2f\nMCC: %0.2f'%(acc, tp, tn, fp, fn, f1, precision, recall, mcc))



def compute_roc_curve(y_test, y_score):
  """
  Calculating the True Positive Rate and False Positiva Rate to assemble the roc_auc curve.
  Printing the Roc Curve and the Auc Score

  Arguments:
  y_test      -- the true classses of the test set obesrvations (np integer array shape [len(y_test), 1])
  y_score_p   -- the probabilities which the classifier assigns to each observation in the test set (np float array shape [1, len(y_test)])
  """
  assert len(y_test) == len(y_score)
  # code taken from sklearn https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html
  # Compute ROC curve and ROC area for each class
  fpr = dict()
  tpr = dict()
  roc_auc = dict()
  for i in range(2):
      fpr[i], tpr[i], _ = roc_curve(y_test.reshape(-1), y_score.reshape(-1))
      roc_auc[i] = auc(fpr[i], tpr[i])

  # Compute micro-average ROC curve and ROC area
  #fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_score.ravel())
  #roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
  print('\nThe ROC AUC curve can be seen below.')
  plt.figure()
  lw = 2
  plt.plot(fpr[1], tpr[1], color='darkorange',
         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[1])
  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
  #plt.plot(fpr[1], tpr[1])
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title('Receiver operating characteristic')
  plt.legend(loc="lower right")
  plt.show()


def evaluate_classifier(y_test, y_score):
  """
  Calls the print_acc_measures and comput_roc_curve functions

  Arguments:
  y_test      -- the true classses of the test set obesrvations (np integer array shape [1, len(y_test)])
  y_score   -- the probabilities which the classifier assigns to each observation in the test set (np float array shape [1, len(y_test)])
  """
  y_test.reshape(1,-1)
  y_score_p = y_score.reshape(1,-1)
  y_score_c = (y_score_p > 0.5).astype(int).reshape(1, -1)

  print_acc_measures(y_test, y_score_c)
  compute_roc_curve(y_test, y_score_p)
